{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import MLP\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import enum, h5py\n",
    "\n",
    "\n",
    "class ActivationFunction(enum.Enum):\n",
    "    SIGMOID = 1\n",
    "    TANH = 2\n",
    "    RELU = 3\n",
    "    LEAKY_RELU = 4\n",
    "    SOFTMAX = 5\n",
    "    LINEAR = 6\n",
    "\n",
    "class LossFunction(enum.Enum):\n",
    "    BCE = 1\n",
    "    MSE = 2\n",
    "    MAE = 3\n",
    "    CROSSENTROPY = 4\n",
    "\n",
    "class Optimizer(enum.Enum):\n",
    "    SGD = 1\n",
    "    ADAM = 2\n",
    "    SGD_MOMENTUM = 3\n",
    "    ADAGRAD = 4\n",
    "    RMSPROP = 5\n",
    "\n",
    "class Initializer(enum.Enum):\n",
    "    HE = 1\n",
    "    XAVIER = 2\n",
    "    NORMAL = 3\n",
    "    UNIFORM = 4\n",
    "\n",
    "class Regularizer(enum.Enum):\n",
    "    L1 = 1\n",
    "    L2 = 2\n",
    "    L1_L2 = 3\n",
    "\n",
    "class LayerType(enum.Enum):\n",
    "    DENSE = 1\n",
    "    FLATTEN = 2\n",
    "    DROPOUT = 3\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, type, shape, activation_function, initializer):\n",
    "        self.type = type\n",
    "        self.shape = shape\n",
    "        self.unit_count = shape[0] * shape[1]\n",
    "        self.activation_function = activation_function\n",
    "        self.units = []\n",
    "        self.initializer = initializer\n",
    "'''\n",
    "class Dense(Layer):\n",
    "    def __init__(self, units, activation_function):\n",
    "        super().__init__(LayerType.DENSE, (1, units), activation_function)\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__(LayerType.FLATTEN, shape, ActivationFunction.LINEAR)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "'''\n",
    "\n",
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        f = ActivationFunctions.sigmoid(x)\n",
    "        return f * (1 - f)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(x):\n",
    "        return np.maximum(0.01 * x, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(x, alpha=0.01):\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative(x):\n",
    "        f = ActivationFunctions.softmax(x)\n",
    "        return f * (1 - f)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(x):\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    \n",
    "class LossFunctions:\n",
    "    @staticmethod\n",
    "    def bce(y, y_hat):\n",
    "        return -np.mean(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y, y_hat):\n",
    "        return np.mean((y - y_hat)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y, y_hat):\n",
    "        return np.mean(np.abs(y - y_hat))\n",
    "\n",
    "    @staticmethod\n",
    "    def crossentropy(y, y_hat):\n",
    "        return -np.mean(y * np.log(y_hat))\n",
    "    \n",
    "class Optimizers:\n",
    "    @staticmethod\n",
    "    def sgd(learning_rate, weights, bias, grad_weights, grad_bias):\n",
    "        weights -= learning_rate * grad_weights\n",
    "        bias -= learning_rate * grad_bias\n",
    "        return weights, bias\n",
    "\n",
    "    @staticmethod\n",
    "    def adam(learning_rate, weights, bias, grad_weights, grad_bias, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        t += 1\n",
    "        m = beta1 * m + (1 - beta1) * grad_weights\n",
    "        v = beta2 * v + (1 - beta2) * grad_bias**2\n",
    "        m_corrected = m / (1 - beta1**t)\n",
    "        v_corrected = v / (1 - beta2**t)\n",
    "        weights -= learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n",
    "        bias -= learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n",
    "        return weights, bias, m, v, t\n",
    "\n",
    "    @staticmethod\n",
    "    def sgd_momentum(learning_rate, weights, bias, grad_weights, grad_bias, beta=0.9):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def adagrad(learning_rate, weights, bias, grad_weights, grad_bias):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def rmsprop(learning_rate, weights, bias, grad_weights, grad_bias, beta=0.9, epsilon=1e-8):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Neuron:\n",
    "    def __init__(self, activation, weights, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        if activation == ActivationFunction.SIGMOID:\n",
    "            self.activation = ActivationFunctions.sigmoid\n",
    "        elif activation == ActivationFunction.TANH:\n",
    "            self.activation = ActivationFunctions.tanh\n",
    "        elif activation == ActivationFunction.RELU:\n",
    "            self.activation = ActivationFunctions.relu\n",
    "        elif activation == ActivationFunction.LEAKY_RELU:\n",
    "            self.activation = ActivationFunctions.leaky_relu\n",
    "        elif activation == ActivationFunction.SOFTMAX:\n",
    "            self.activation = ActivationFunctions.softmax\n",
    "        elif activation == ActivationFunction.LINEAR:\n",
    "            self.activation = ActivationFunctions.linear\n",
    "        else:\n",
    "            raise Exception(f'Invalid activation function: {activation}')\n",
    "\n",
    "    def compute(self, input):\n",
    "        self.input = input\n",
    "        self.output = self.activation(np.dot(input.T, self.weights.T) + self.bias)[0]\n",
    "        return self.output\n",
    "    \n",
    "\n",
    "class Model:\n",
    "    def __init__(self, loss_function, optimizer, regularizer, dropout_rate=0.0):\n",
    "        self.layers = []\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.regularizer = regularizer\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, batch_size, learning_rate):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batch_size):\n",
    "                start = batch * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                x_batch = x_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                for i in range(len(x_batch)):\n",
    "                    y_hat = self.__forward(x_batch[i])\n",
    "\n",
    "                    if self.loss_function == LossFunction.BCE:\n",
    "                        loss = LossFunctions.bce(y_batch[i], y_hat)\n",
    "                    elif self.loss_function == LossFunction.MSE:\n",
    "                        loss = LossFunctions.mse(y_batch[i], y_hat)\n",
    "                    elif self.loss_function == LossFunction.MAE:\n",
    "                        loss = LossFunctions.mae(y_batch[i], y_hat)\n",
    "                    elif self.loss_function == LossFunction.CROSSENTROPY:\n",
    "                        loss = LossFunctions.crossentropy(y_batch[i], y_hat)\n",
    "                    \n",
    "                    self.__backward(x_batch[i], y_batch[i], y_hat, learning_rate)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs} - loss: {loss}')\n",
    "\n",
    "    def __forward(self, x_batch):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            input = []\n",
    "            if i==0:\n",
    "                for j, neuron in enumerate(layer.units):\n",
    "                    neuron.compute(x_batch[j])\n",
    "            else:\n",
    "                input = []\n",
    "                for j in range(len(self.layers[i-1].units)):\n",
    "                    input.append(self.layers[i-1].units[j].output)\n",
    "                \n",
    "                input = np.array(input)\n",
    "                for neuron in layer.units:\n",
    "                    neuron.compute(input)\n",
    "\n",
    "        y_hat = []\n",
    "        for j in range(len(self.layers[-1].units)):\n",
    "            y_hat.append(self.layers[-1].units[j].output)\n",
    "\n",
    "        return np.array(y_hat).T[0]\n",
    "\n",
    "    def __backward(self, x_batch, y_batch, y_hat, learning_rate):\n",
    "        m = y_batch.shape[0]\n",
    "\n",
    "        dA = np.divide(y_batch, y_hat) - np.divide(1 - y_batch, 1 - y_hat)\n",
    "\n",
    "        for i, layer in reversed(list(enumerate(self.layers))):\n",
    "            if layer.activation_function == ActivationFunction.SIGMOID:\n",
    "                dZ = dA * ActivationFunctions.sigmoid_derivative(y_hat)\n",
    "            elif layer.activation_function == ActivationFunction.TANH:\n",
    "                dZ = dA * ActivationFunctions.tanh_derivative(y_hat)\n",
    "            elif layer.activation_function == ActivationFunction.RELU:\n",
    "                dZ = dA * ActivationFunctions.relu_derivative(y_hat)\n",
    "            elif layer.activation_function == ActivationFunction.LEAKY_RELU:\n",
    "                dZ = dA * ActivationFunctions.leaky_relu_derivative(y_hat)\n",
    "            elif layer.activation_function == ActivationFunction.SOFTMAX:\n",
    "                dZ = dA * ActivationFunctions.softmax_derivative(y_hat)\n",
    "            elif layer.activation_function == ActivationFunction.LINEAR:\n",
    "                dZ = dA * ActivationFunctions.linear_derivative(y_hat)\n",
    "\n",
    "            dW = 1/m * np.dot(dZ, x_batch.T)\n",
    "            db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            if self.optimizer == Optimizer.SGD:\n",
    "                for neuron in layer.units:\n",
    "                    neuron.weights, neuron.bias = Optimizers.sgd(learning_rate, neuron.weights, neuron.bias, dW, db)\n",
    "            elif self.optimizer == Optimizer.ADAM:\n",
    "                for neuron in layer.units:\n",
    "                    neuron.weights, neuron.bias, neuron.m, neuron.v, neuron.t = Optimizers.adam(learning_rate, neuron.weights, neuron.bias, dW, db, neuron.m, neuron.v, neuron.t)\n",
    "\n",
    "            dA = np.dot(dW.T, dZ)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        pass\n",
    "\n",
    "    def summary(self):\n",
    "        print('Number of layers: ', len(self.layers))\n",
    "\n",
    "        units = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            units += layer.shape[0] * layer.shape[1]\n",
    "        print('Units: ', units)\n",
    "\n",
    "        parameters = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.type == LayerType.FLATTEN:\n",
    "                parameters += 0\n",
    "            else:\n",
    "                parameters += len(layer.units) # bias\n",
    "                parameters += (layer.units[0].weights.shape[0] * layer.units[0].weights.shape[1]) * len(layer.units)\n",
    "        print('Number of parameters: ', parameters)\n",
    "\n",
    "        print('Loss function: ', self.loss_function) \n",
    "        print('Optimizer: ', self.optimizer)\n",
    "        print('Regularizer: ', self.regularizer)        \n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('Initializer: ', layer.initializer)\n",
    "            print('Layer ', i, layer.shape[0] * layer.shape[1], ' neurons')\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('Layer ', i, layer.shape[1], ' neurons')\n",
    "            for j, neuron in enumerate(layer.units):\n",
    "                print('\\tNeuron ', j, neuron.weights.shape, ' weights')\n",
    "                print('\\tWeights ', neuron.weights)\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.initializer == Initializer.HE:\n",
    "                self.__initialization_HE(i, layer)\n",
    "            elif layer.initializer == Initializer.XAVIER:\n",
    "                self.__initialization_XAVIER(i, layer)\n",
    "            elif layer.initializer == Initializer.NORMAL:\n",
    "                self.__initialization_NORMAL(i, layer)\n",
    "            elif layer.initializer == Initializer.UNIFORM:\n",
    "                self.__initialization_UNIFORM(i, layer)\n",
    "            else:\n",
    "                raise Exception(f'Invalid initializer: {self.initializer}')\n",
    "\n",
    "\n",
    "    def __initialization_HE(self, i, layer):\n",
    "        for a in range(layer.shape[0]):\n",
    "            for b in range(layer.shape[1]):\n",
    "                if i == 0:\n",
    "                    weights = np.ones((1,1))\n",
    "                else:\n",
    "                    unit_count = self.layers[i-1].unit_count\n",
    "                    weights = np.random.randn(self.layers[i-1].shape[0], self.layers[i-1].shape[1]) * np.sqrt(2 / unit_count)\n",
    "                    \n",
    "                self.layers[i].units.append(Neuron(layer.activation_function, weights, 1))\n",
    "\n",
    "    def __initialization_XAVIER(self, i, layer):\n",
    "        for a in range(layer.shape[0]):\n",
    "            for b in range(layer.shape[1]):\n",
    "                if i == 0:\n",
    "                    weights = np.ones((1,1))\n",
    "                else:\n",
    "                    unit_count = self.layers[i-1].unit_count + layer.unit_count\n",
    "                    weights = np.random.randn(self.layers[i-1].shape[0], self.layers[i-1].shape[1]) * np.sqrt(2 / unit_count)\n",
    "                    \n",
    "                self.layers[i].units.append(Neuron(layer.activation_function, weights, 1))\n",
    "\n",
    "    def __initialization_NORMAL(self, i, layer):\n",
    "        for a in range(layer.shape[0]):\n",
    "            for b in range(layer.shape[1]):\n",
    "                if i == 0:\n",
    "                    weights = np.ones((1,1))\n",
    "                else:\n",
    "                    weights = np.random.randn(self.layers[i-1].shape[0], self.layers[i-1].shape[1]) * np.sqrt(0.01)\n",
    "                \n",
    "                self.layers[i].units.append(Neuron(layer.activation_function, weights, 1))\n",
    "\n",
    "    def __initialization_UNIFORM(self, i, layer):\n",
    "        for a in range(layer.shape[0]):\n",
    "            for b in range(layer.shape[1]):\n",
    "                if i == 0:\n",
    "                    weights = np.ones((1,1))\n",
    "                else:\n",
    "                    weights = np.random.uniform(-0.01, 0.01, self.layers[i-1].shape)\n",
    "                \n",
    "                self.layers[i].units.append(Neuron(layer.activation_function, weights, 1))\n",
    "    \n",
    "    def save(self, path):\n",
    "        with h5py.File(path, 'w') as f:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                f.create_dataset('layer' + str(i), data=layer)\n",
    "            \n",
    "            f.create_dataset('loss_function', data=self.loss_function)\n",
    "            f.create_dataset('optimizer', data=self.optimizer)\n",
    "            f.create_dataset('initializer', data=self.initializer)\n",
    "            f.create_dataset('regularizer', data=self.regularizer)\n",
    "\n",
    "            for i, (w, b) in enumerate(self.weights):\n",
    "                f.create_dataset('weights' + str(i), data=w)\n",
    "                f.create_dataset('bias' + str(i), data=b)\n",
    "\n",
    "    def load(self, path):\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                layer = f['layer' + str(i)].value\n",
    "                self.layers.append(layer)\n",
    "\n",
    "            self.loss_function = f['loss_function'].value\n",
    "            self.optimizer = f['optimizer'].value\n",
    "            self.initializer = f['initializer'].value\n",
    "            self.regularizer = f['regularizer'].value\n",
    "\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                w = f['weights' + str(i)].value\n",
    "                b = f['bias' + str(i)].value\n",
    "                self.weights.append((w, b))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(LossFunction.MSE, Optimizer.ADAM, Regularizer.L2)\n",
    "\n",
    "model.add(Layer(LayerType.FLATTEN, (1,4), ActivationFunction.RELU, Initializer.HE))\n",
    "model.add(Layer(LayerType.DENSE, (1,3), ActivationFunction.RELU, Initializer.UNIFORM))\n",
    "model.add(Layer(LayerType.DENSE, (1,2), ActivationFunction.SOFTMAX, Initializer.NORMAL))\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\crist\\GitHub\\PUCRS\\2023-02\\98H09-04 - Aprendizado Profundo I\\T1\\tests.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m10\u001b[39;49m, \u001b[39m100\u001b[39;49m, \u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\crist\\GitHub\\PUCRS\\2023-02\\98H09-04 - Aprendizado Profundo I\\T1\\tests.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m         \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function \u001b[39m==\u001b[39m LossFunction\u001b[39m.\u001b[39mCROSSENTROPY:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m             loss \u001b[39m=\u001b[39m LossFunctions\u001b[39m.\u001b[39mcrossentropy(y_batch[i], y_hat)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=221'>222</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__backward(x_batch[i], y_batch[i], y_hat, learning_rate)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m - loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\crist\\GitHub\\PUCRS\\2023-02\\98H09-04 - Aprendizado Profundo I\\T1\\tests.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=246'>247</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__backward\u001b[39m(\u001b[39mself\u001b[39m, x_batch, y_batch, y_hat, learning_rate):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=247'>248</a>\u001b[0m     m \u001b[39m=\u001b[39m y_batch\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m     dA \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdivide(y_batch, y_hat) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mdivide(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_batch, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_hat)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/crist/GitHub/PUCRS/2023-02/98H09-04%20-%20Aprendizado%20Profundo%20I/T1/tests.ipynb#W6sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers))):\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, 10, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16817962]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([[[8.2]], [[4.6]], [[7.1]], [[3.5]]])\n",
    "weights = np.array([[0.00863463, 0.00525933, 0.00761537, 0.00546103]])\n",
    "\n",
    "\n",
    "np.dot(input.T[0], weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2, 4.6, 7.1, 3.5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1 = (1,2)\n",
    "shape2 = (1,4)\n",
    "weights = np.random.randn(shape2[0], shape2[1]) * np.sqrt(2 / shape1[1])\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in enumerate(shape):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(-0.01, 0.01, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones_like((1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
