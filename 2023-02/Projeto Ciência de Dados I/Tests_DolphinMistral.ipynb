{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\crist\\.conda\\envs\\llama2\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\crist\\.conda\\envs\\llama2\\lib\\site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\crist\\.conda\\envs\\llama2\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crist\\.conda\\envs\\llama2\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\crist\\.conda\\envs\\llama2\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crist\\.conda\\envs\\llama2\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Downloading tiktoken-0.5.1-cp310-cp310-win_amd64.whl (759 kB)\n",
      "   ---------------------------------------- 0.0/759.8 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/759.8 kB 660.6 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 122.9/759.8 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 450.6/759.8 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 759.8/759.8 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "#import utils\n",
    "\n",
    "#from google.colab import drive\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_hf = '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'\n",
    "path='D:/Disco/Data/huggingface/'\n",
    "reviews_path = 'D:/Disco/Data/datasets/amazon_us_reviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "\n",
    "    def load_reviews(self, file_name):\n",
    "        self.reviews = pd.read_parquet(reviews_path + file_name)\n",
    "\n",
    "    def get_reviews_by_product_and_category(self, product, category):\n",
    "        return self.reviews[(self.reviews['product_category'] == category) & (self.reviews['product_title'] == product)]\n",
    "\n",
    "    def get_prompt(self, product):\n",
    "        return f\"Please analyze the provided reviews of the product '{product}'. \\n  \\\n",
    "        Create a concise summary that encapsulates the key opinions and sentiments expressed in these reviews. \\n \\\n",
    "        The summary should be structured as if it's a single comprehensive review of the product. \\n \\\n",
    "        The summary should mimic the style and tone of a customer reviews, making it relatable and genuine. \\n \\\n",
    "        Also, provide a list of 5 tags that represent what the customers are saying about the product, give the balance between positive and negative aspects about the product, the tags have this format: #TagName. \\\n",
    "        Format your response as follows: \\n \\\n",
    "            Product: {product}\\n \\\n",
    "            Summary: [Your summary here]\\n \\\n",
    "            Tags: #tag1 #tag2 #tag3 #tag4 #tag5\\n \\\n",
    "        List of reviews: \\n\"\n",
    "    \n",
    "    def build_summary(self, review_qty=50, model='gpt-4'):\n",
    "        results = pd.DataFrame(columns=['category', 'product', 'prompt', 'reviews', 'review_qty', 'token_qty', 'response'])\n",
    "\n",
    "        for row, qty in self.reviews[['product_category', 'product_title']].value_counts()[:10].items():\n",
    "            try:\n",
    "                print(row[0], row[1], qty)\n",
    "                category = row[0]\n",
    "                product = row[1]\n",
    "\n",
    "                product_reviews = self.reviews[(self.reviews['product_category'] == category) & (self.reviews['product_title'] == product)][:review_qty]\n",
    "                \n",
    "                reviews = '\\n'.join(product_reviews['review_body'])\n",
    "                prompt = self.get_prompt(product)\n",
    "\n",
    "                message_prompt=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{prompt + reviews}:{product_reviews['review_body']}\"}]\n",
    "                \n",
    "                token_qty = num_tokens_from_messages(message_prompt, model=model)\n",
    "                print('tokens: ', token_qty)\n",
    "\n",
    "                chat_completion = openai.ChatCompletion.create(model=model, messages=message_prompt)\n",
    "                response = chat_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "                results = results.append({'category': category, 'product': product, 'prompt': prompt, 'reviews': reviews, 'review_qty':len(product_reviews), 'token_qty': token_qty, 'response': response}, ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print('error processing: ', row[0], row[1], qty)\n",
    "                pass\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:11<00:00, 35.71s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ehartford_dolphin-2.2.1-mistral-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path + model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:50<00:00, 55.39s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline_llm = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=path + model_name,\n",
    "    torch_dtype=torch.float32,#16 is GPU, 32 is CPU\n",
    "    #device_map=\"auto\"#, \n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm(prompt_text):\n",
    "  sequences = pipeline_llm(\n",
    "      prompt_text,\n",
    "      do_sample=False,\n",
    "      top_k=0,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "      max_length=1000,\n",
    "  )\n",
    "\n",
    "  result = ('Result from llm:')\n",
    "  \n",
    "  for seq in sequences:\n",
    "      result += (f\"Result: {seq['generated_text']}\")\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utl = Utils()\n",
    "utl.load_reviews('reviews_aws_sample.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Watches'\n",
    "product = 'Timex Women\\'s Easy Reader Leather Strap Watch'\n",
    "\n",
    "product_reviews = utl.reviews[(utl.reviews['product_category'] == category) & (utl.reviews['product_title'] == product)]\n",
    "\n",
    "prompt = utl.get_prompt(product) + '\\n'.join(product_reviews['review_body'][:])\n",
    "result = prompt_llm(prompt)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please analyze the provided reviews of the product 'Timex Women's Easy Reader Leather Strap Watch'. \n",
      "          Create a concise summary that encapsulates the key opinions and sentiments expressed in these reviews. \n",
      "         The summary should be structured as if it's a single comprehensive review of the product. \n",
      "         The summary should mimic the style and tone of a customer reviews, making it relatable and genuine. \n",
      "         Also, provide a list of 5 tags that represent what the customers are saying about the product, give the balance between positive and negative aspects about the product, the tags have this format: #TagName.         Format your response as follows: \n",
      "             Product: Timex Women's Easy Reader Leather Strap Watch\n",
      "             Summary: [Your summary here]\n",
      "             Tags: #tag1 #tag2 #tag3 #tag4 #tag5\n",
      "         List of reviews: \n",
      "Purchased as a gift for my elderly Mom.  She loved it.  What more can I add.\n",
      "Nice !\n",
      "Timex offers a wide variety of materials, colors, lengths and widths of bands, which makes the choice difficult. I am pleased with this watch because it is a great color and its dimensions are suitable for what I wanted.\n",
      "Love the oversize face!  The band is a bit stiff, but should mold to my wrist as I wear it more.  It is a good looking watch!  I am happy to see the numerals without my glasses!  Very glad I bought it.\n",
      "Love it!!!\n",
      "Easy to read and looks great, but runs slow. When I tried to reset the time, I accidentally pulled the stem all the way out. It's tricky to pull the stem out just the right amount for resetting. The watch repairman says it would cost more to fix than to replace.\n",
      "Large face is very easy to read.  Indigo light is great in the dark\n",
      "The watch looks just like the one in the picture. It seems a little heavy on the wrist, but that hasn't kept me from wearing it. The color is the same as in the picture.\n",
      "I purchased this watch for couple of months ago. I have worn it daily, but not at night and never near water.  Within two weeks, I noticed that the attractive blue leather watchband was losing color. Within the first month, it was noticeably faded in large patches. At this point, about two months after purchase, the watchband looks so bad that I need to get it replaced. That having been said, the watch face itself is very attractive and has kept great time. Nonetheless, you should think twice before buying this watch with this particular band. You will be replacing the band shortly after purchase.\n",
      "Don't get this wet! It will temporarily stop working. The first time this happened, my watch was about 1 hour slow. I corrected the time, then it seemed to work fine. But If I wash my hands and water gets near it, it will slow down again...\n",
      "This was bought as a Christmas gift. The recipient is very happy with it. I seem to always trust timex to have a nice affordable watch.\n",
      "Just what I wanted, clear face, glows in the dark with a red band... and it came on time.\n"
     ]
    }
   ],
   "source": [
    "print(utl.get_prompt(product) + '\\n'.join(product_reviews['review_body'][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crist\\.conda\\envs\\llama2\\lib\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "c:\\Users\\crist\\.conda\\envs\\llama2\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Result from llm:Result: Please analyze the provided reviews of the product 'Timex Women's Easy Reader Leather Strap Watch'. \\n          Create a concise summary that encapsulates the key opinions and sentiments expressed in these reviews. \\n         The summary should be structured as if it's a single comprehensive review of the product. \\n         The summary should mimic the style and tone of a customer reviews, making it relatable and genuine. \\n         Also, provide a list of 5 tags that represent what the customers are saying about the product, give the balance between positive and negative aspects about the product, the tags have this format: #TagName.         Format your response as follows: \\n             Product: Timex Women's Easy Reader Leather Strap Watch\\n             Summary: [Your summary here]\\n             Tags: #tag1 #tag2 #tag3 #tag4 #tag5\\n         List of reviews: \\nPurchased as a gift for my elderly Mom.  She loved it.  What more can I add.\\nNice !\\nTimex offers a wide variety of materials, colors, lengths and widths of bands, which makes the choice difficult. I am pleased with this watch because it is a great color and its dimensions are suitable for what I wanted.\\nLove the oversize face!  The band is a bit stiff, but should mold to my wrist as I wear it more.  It is a good looking watch!  I am happy to see the numerals without my glasses!  Very glad I bought it.\\nLove it!!!\\nEasy to read and looks great, but runs slow. When I tried to reset the time, I accidentally pulled the stem all the way out. It's tricky to pull the stem out just the right amount for resetting. The watch repairman says it would cost more to fix than to replace.\\nLarge face is very easy to read.  Indigo light is great in the dark\\nThe watch looks just like the one in the picture. It seems a little heavy on the wrist, but that hasn't kept me from wearing it. The color is the same as in the picture.\\nI purchased this watch for couple of months ago. I have worn it daily, but not at night and never near water.  Within two weeks, I noticed that the attractive blue leather watchband was losing color. Within the first month, it was noticeably faded in large patches. At this point, about two months after purchase, the watchband looks so bad that I need to get it replaced. That having been said, the watch face itself is very attractive and has kept great time. Nonetheless, you should think twice before buying this watch with this particular band. You will be replacing the band shortly after purchase.\\nDon't get this wet! It will temporarily stop working. The first time this happened, my watch was about 1 hour slow. I corrected the time, then it seemed to work fine. But If I wash my hands and water gets near it, it will slow down again...\\nThis was bought as a Christmas gift. The recipient is very happy with it. I seem to always trust timex to have a nice affordable watch.\\nJust what I wanted, clear face, glows in the dark with a red band... and it came on time.\\n\\nProduct: Timex Women's Easy Reader Leather Strap Watch\\nSummary: This watch is a great gift for elderly people due to its large, easy-to-read face. The leather strap is comfortable and stylish, but may fade quickly. The watch is water-resistant, but should not be submerged in water. The watch is affordable and reliable, making it a good choice for those looking for a simple, functional timepiece.\\nTags: #Easy-to-read #Leather-strap #Affordable #Water-resistant #Reliable\\n```\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = utl.get_prompt(product) + '\\n'.join(product_reviews['review_body'][:])\n",
    "prompt_llm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
