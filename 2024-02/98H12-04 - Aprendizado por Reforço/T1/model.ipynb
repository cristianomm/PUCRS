{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.26.4\n",
    "#!pip install highway-env\n",
    "#!pip install git+https://github.com/DLR-RM/stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random, time, sys\n",
    "\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium  import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridLineEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Ambiente personalizado onde o agente deve percorrer uma linha em uma grade NxN.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, grid_size=10, line_length=10, line_reward=1.0, goal_reward=10.0, gray_penalty=-2.0, walk_penalty=-1.0):\n",
    "        super(GridLineEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.line_length = line_length\n",
    "        self.line_reward = line_reward\n",
    "        self.goal_reward = goal_reward\n",
    "        self.gray_penalty = gray_penalty\n",
    "\n",
    "        # Definir espaço de ação: 8 direções\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        # Definir espaço de observação: posição do agente na grade\n",
    "        self.observation_space = spaces.MultiDiscrete([grid_size, grid_size])\n",
    "\n",
    "        # Configuração da linha\n",
    "        self.line_positions = []\n",
    "        for i in range(line_length):\n",
    "            self.line_positions.append((i+1, grid_size // 2))  # Linha horizontal no meio da grade\n",
    "\n",
    "        self.start_position = self.line_positions[0]\n",
    "        self.end_position = self.line_positions[-1]\n",
    "        self.state = self.start_position\n",
    "\n",
    "        # Recompensas e penalidades\n",
    "        self.rewards = {\n",
    "            'line': line_reward,\n",
    "            'goal': goal_reward,\n",
    "            'gray': gray_penalty,\n",
    "            'walk': walk_penalty\n",
    "        }\n",
    "\n",
    "        # Configuração do pygame\n",
    "        self.window_size = 400  # Tamanho da janela\n",
    "        self.cell_size = self.window_size // self.grid_size\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_position\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        # Movimentos possíveis\n",
    "        moves = {\n",
    "            0: (-1,  0),  # Esquerda\n",
    "            1: (-1, -1),  # Diagonal superior esquerda\n",
    "            2: ( 0, -1),  # Cima\n",
    "            3: ( 1, -1),  # Diagonal superior direita\n",
    "            4: ( 1,  0),  # Direita\n",
    "            5: ( 1,  1),  # Diagonal inferior direita\n",
    "            6: ( 0,  1),  # Baixo\n",
    "            7: (-1,  1),  # Diagonal inferior esquerda\n",
    "        }\n",
    "\n",
    "        move = moves.get(action, (0, 0))\n",
    "        new_state = (self.state[0] + move[0], self.state[1] + move[1])\n",
    "\n",
    "        # Verificar limites da grade\n",
    "        if 0 <= new_state[0] < self.grid_size and 0 <= new_state[1] < self.grid_size:\n",
    "            self.state = new_state\n",
    "        else:\n",
    "            # Se bater na parede, permanece no mesmo lugar\n",
    "            pass\n",
    "\n",
    "        # Calcular recompensa\n",
    "        if self.state == self.end_position:\n",
    "            reward = self.rewards['goal']\n",
    "            done = True\n",
    "        elif self.state in self.line_positions:\n",
    "            reward = self.rewards['line'] + self.rewards['walk']\n",
    "        else:\n",
    "            reward = self.rewards['gray'] + self.rewards['walk']\n",
    "\n",
    "        return np.array(self.state), reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "            pygame.display.set_caption(\"Grid Line Environment\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "        # Desenhar fundo\n",
    "        self.screen.fill((128, 128, 128))  # Cinza\n",
    "\n",
    "        # Desenhar linha\n",
    "        for pos in self.line_positions:\n",
    "            x, y = pos\n",
    "            rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (0, 0, 0), rect)  # Preto\n",
    "\n",
    "        # Desenhar objetivo\n",
    "        x, y = self.end_position\n",
    "        rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (255, 255, 0), rect)  # Amarelo\n",
    "\n",
    "        # Desenhar agente\n",
    "        x, y = self.state\n",
    "        rect = pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), rect)  # Vermelho\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(5)  # Controla a velocidade da renderização\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloAgent:\n",
    "    \"\"\"\n",
    "    Agente que utiliza o método de Controle Monte Carlo com Exploração-First.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.states = [(x, y) for x in range(env.grid_size) for y in range(env.grid_size)]\n",
    "        self.actions = list(range(env.action_space.n))\n",
    "        self.gamma = 0.9  # Fator de desconto\n",
    "        self.epsilon = epsilon  # Parâmetro para ε-greedy\n",
    "\n",
    "        # Inicializa Q(s, a) e as listas de retornos\n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                self.Q[(state, action)] = 0.0\n",
    "                self.returns[(state, action)] = []\n",
    "\n",
    "        # Política inicial (aleatória)\n",
    "        self.policy = {}\n",
    "        for state in self.states:\n",
    "            self.policy[state] = np.ones(env.action_space.n) / env.action_space.n\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Escolhe uma ação aleatória (exploração)\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            # Escolhe a melhor ação com base em Q(s, a) (exploração)\n",
    "            q_values = [self.Q.get((state, a), 0.0) for a in self.actions]\n",
    "            max_q = np.max(q_values)\n",
    "            # Pode haver várias ações com o mesmo valor máximo\n",
    "            best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def generate_episode(self):\n",
    "        episode = []\n",
    "        state = tuple(self.env.reset())\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = tuple(next_state)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        return episode\n",
    "\n",
    "    def update_policy(self):\n",
    "        for state in self.states:\n",
    "            q_values = [self.Q.get((state, a), 0.0) for a in self.actions]\n",
    "            max_q = np.max(q_values)\n",
    "            best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]\n",
    "            # Política ε-greedy\n",
    "            self.policy[state] = np.ones(len(self.actions)) * self.epsilon / len(self.actions)\n",
    "            for a in best_actions:\n",
    "                self.policy[state][a] += (1.0 - self.epsilon) / len(best_actions)\n",
    "\n",
    "    def monte_carlo_control(self, episodes=1000):\n",
    "        for _ in range(episodes):\n",
    "            episode = self.generate_episode()\n",
    "            G = 0\n",
    "            visited_state_action_pairs = set()\n",
    "            for state, action, reward in reversed(episode):\n",
    "                G = reward + self.gamma * G\n",
    "                if (state, action) not in visited_state_action_pairs:\n",
    "                    self.returns[(state, action)].append(G)\n",
    "                    self.Q[(state, action)] = np.mean(self.returns[(state, action)])\n",
    "                    visited_state_action_pairs.add((state, action))\n",
    "            # Atualiza a política após cada episódio\n",
    "            self.update_policy()\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        self.monte_carlo_control(episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating environment...\n",
      "training...\n",
      "Episódio 1: Recompensa total = -14.0\n",
      "Episódio 2: Recompensa total = -23.0\n",
      "Episódio 3: Recompensa total = -12.0\n",
      "Episódio 4: Recompensa total = -18.0\n",
      "Episódio 5: Recompensa total = -9.0\n",
      "Episódio 6: Recompensa total = -1.0\n",
      "Episódio 7: Recompensa total = -19.0\n",
      "Episódio 8: Recompensa total = -2.0\n",
      "Episódio 9: Recompensa total = 2.0\n",
      "Episódio 10: Recompensa total = -18.0\n",
      "Episódio 11: Recompensa total = -10.0\n",
      "Episódio 12: Recompensa total = -9.0\n",
      "Episódio 13: Recompensa total = -15.0\n",
      "Episódio 14: Recompensa total = -11.0\n",
      "Episódio 15: Recompensa total = -1.0\n",
      "Episódio 16: Recompensa total = -15.0\n",
      "Episódio 17: Recompensa total = -38.0\n",
      "Episódio 18: Recompensa total = -38.0\n",
      "Episódio 19: Recompensa total = -21.0\n",
      "Episódio 20: Recompensa total = -9.0\n",
      "Episódio 21: Recompensa total = -2.0\n",
      "Episódio 22: Recompensa total = -10.0\n",
      "Episódio 23: Recompensa total = -31.0\n",
      "Episódio 24: Recompensa total = -4.0\n",
      "Episódio 25: Recompensa total = -9.0\n",
      "Episódio 26: Recompensa total = -15.0\n",
      "Episódio 27: Recompensa total = -8.0\n",
      "Episódio 28: Recompensa total = -6.0\n",
      "Episódio 29: Recompensa total = -3.0\n",
      "Episódio 30: Recompensa total = -29.0\n",
      "Episódio 31: Recompensa total = -77.0\n",
      "Episódio 32: Recompensa total = -26.0\n",
      "Episódio 33: Recompensa total = -27.0\n",
      "Episódio 34: Recompensa total = 0.0\n",
      "Episódio 35: Recompensa total = -5.0\n",
      "Episódio 36: Recompensa total = -47.0\n",
      "Episódio 37: Recompensa total = -3.0\n",
      "Episódio 38: Recompensa total = -25.0\n",
      "Episódio 39: Recompensa total = 2.0\n",
      "Episódio 40: Recompensa total = -20.0\n",
      "Episódio 41: Recompensa total = -7.0\n",
      "Episódio 42: Recompensa total = -33.0\n",
      "Episódio 43: Recompensa total = 1.0\n",
      "Episódio 44: Recompensa total = -72.0\n",
      "Episódio 45: Recompensa total = -14.0\n",
      "Episódio 46: Recompensa total = -7.0\n",
      "Episódio 47: Recompensa total = -70.0\n",
      "Episódio 48: Recompensa total = -26.0\n",
      "Episódio 49: Recompensa total = 3.0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Código para executar o ambiente e o agente\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurações\n",
    "    grid_size = 30\n",
    "    line_length = 15\n",
    "    line_reward = .5\n",
    "    goal_reward = 10.0\n",
    "    gray_penalty = -.5\n",
    "    walk_penalty = -.5\n",
    "    episodes = 100  # Número de episódios para treinamento\n",
    "\n",
    "    print('creating environment...')\n",
    "    env = GridLineEnv(grid_size=grid_size, line_length=line_length,\n",
    "                      line_reward=line_reward, goal_reward=goal_reward,\n",
    "                      gray_penalty=gray_penalty, walk_penalty=walk_penalty)\n",
    "\n",
    "    agent = MonteCarloAgent(env)\n",
    "    print('training...')\n",
    "    agent.train(episodes=episodes)\n",
    "\n",
    "    # Executar episódios com renderização\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            #print('rendering...')\n",
    "            env.render()\n",
    "            state_tuple = tuple(state)\n",
    "            action_probs = agent.policy[state_tuple]\n",
    "            action = np.random.choice(agent.actions, p=action_probs)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episódio {episode + 1}: Recompensa total = {total_reward}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
